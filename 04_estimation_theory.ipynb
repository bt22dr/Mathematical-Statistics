{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 추정이론 \n",
    "\n",
    "연구 대상인 확률변수 X의 확률분포를 $P_\\theta(x), \\theta \\in \\Omega$로 표현하자. 이 경우 모수 $\\theta$는 $\\theta = (\\theta_1, \\theta_2, \\cdots , \\theta_k)$로 표현되는 벡터일 수 있으며(ex. 정규분포의 경우 $\\theta = (\\mu, \\sigma^2)$인 것처럼), $\\Omega$는 모수가 가질 수 있는 값의 집합, 즉 모수공간을 뜻한다. 이제 확률분포 $P_\\theta(x), \\theta \\in \\Omega$로부터 크기가 n인 표본 $X_1, X_2, \\cdots , X_n$을 얻었다고 하자(편의상 $X = X_1, X_2, \\cdots , X_n$로 표기). 이때 우리가 관심이 있는 문제는 확률분포 $P_\\theta(x), \\theta \\in \\Omega$을 특징짓는 모수 $\\theta$ 또는 $\\theta$의 함수인 $g(\\theta)$의 추정이다. \n",
    "\n",
    "$g(\\theta)$의 추정에는 두 가지 방법이 있다. \n",
    "- 점추정(point estimation): 표본에 근거한 통계량 $T(X)=T(X_1, X_2, \\cdots , X_n)$을 사용하여 하나의 값으로 $g(\\theta)$를 추정\n",
    "- 구간추정(interval estimation): 두 개의 통계량 $T_1(X), T_2(X)$을 사용하여 구간 $[T_1(X), T_2(X)]$ 안에 $g(\\theta)$가 포함될 확률을 고려\n",
    "\n",
    "**\\[정의 4.1\\]**  \n",
    "미지의(unknown) 모수를 포함하지 않는 랜덤표본 $X_1, X_2, \\cdots , X_n$의 함수를 통계량(statistic)이라고 한다. 모수 $\\theta$의 함수 $g(\\theta)$를 추정하기 위해 사용되는 통계량 $T(X)=T(X_1, \\cdots , X_n)$을 $g(\\theta)$의 추정량(estimator)이라고 하며, 주어진 표본값 $X_1=x_1, X_2 = x_2, \\cdots, X_n=x_n$을 대입해서 구해진 추정량의 특정값, $T(x)=T(x_1, x_2, \\cdots , x_n)$을 추정값(estimate)이라고 한다. \n",
    "\n",
    "모수 $\\theta$와 구별하기 편리하도록 추정량을 $\\hat{\\theta}$로 표기한다. \n",
    "\n",
    "추정량과 추정값을 예를 들어 설명해보자. $X_1, X_2, \\cdots , X_n$을 $N(\\mu, \\sigma^2)$으로부터 얻은 랜덤표본이라고 하자. 모평균 $\\mu$와 모분산 $\\sigma^2$을 추정하고자 할 때, 표본평균\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = T_1(X_1, X_2, \\cdots , X_n) = \\frac{\\sum_{i=1}^{n}X_i}{n}\n",
    "$$\n",
    "\n",
    "와 표본분산\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = T_2(X_1, X_2, \\cdots , X_n) = \\frac{\\sum_{i=1}^{n} (X_i - \\overline{X}_n)^2}{n-1}\n",
    "$$\n",
    "\n",
    "을 추정량으로 사용할 수 있으며, 표본이 주어졌을 때 그들의 값\n",
    "\n",
    "$$\n",
    "T_1(x_1, x_2, \\cdots , x_n) = \\frac{\\sum_{i=1}^{n}x_i}{n}, \\quad T_2(x_1, x_2, \\cdots , x_n) = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x}_n)^2}{n-1}\n",
    "$$\n",
    "\n",
    "을 추정값이라고 한다. \n",
    "\n",
    "**용어 정리**  \n",
    "- 통계적 추론(Statistical Inference): 모집단에서 추출한 표본 특성을 분석하여, 모집단에 대한 어떤 미지의 양상 or 특징을 알기 위해 통계학을 이용하여 추측하는 과정\n",
    "  - 추정이론(estimation theory): 표본을 통해 모집단 특성(모수)이 어떠한가에 대해 추측하는 과정\n",
    "  - 검정이론(testing theory): 모집단 실제값이 얼마나 되는가 하는 주장과 관련해서, 표본이 가지고 있는 정보를 이용해 가설이 올바른지 그렇지 않은지 판정하는 과정\n",
    "- (통계적) 추정(Statistical Estimation): \n",
    "    - 모수모형일 때: 표본을 통해 모집단의 모수를 알아내는 과정. 예를 들어 한국인의 평균키가 얼마인가? 자동차 사고날 횟수(포아송분포) 예측 등등...\n",
    "- 모수 모형(Parametric Model): 유한한 개수의 모수를 사용하여 설명이 가능한 분포 (cf. 건수가 몇개 안 될 때는 비모수 모형을 사용하는데, 왜냐하면 표본이 너무 작으면 그 표본이 어떤 확률분포를 따른다고 말할 수 있는 근거가 부족하기 때문)\n",
    "- 모수(Parameter): 모집단 분포의 특성을 규정짓는 척도. ('미지'의 '상수'로 취급) 반면 베이즈통계에서는 모수도 변수로 취급한다.\n",
    "- 점추정(Point Estimation): 표본 통계량을 사용하여 하나의 값으로 모수(나 모수의 함수)를 추정하는 방법\n",
    "- 구간추정(Interval Estimation): 두 개의 표본 통계량을 사용하여 그 두 개의 통계량 구간에 모수가 포함될 확률을 고려하는 방법\n",
    "- 통계량(Statistic): 랜덤표본의 확률변수로 만들어지고 모수를 포함하지 않는 확률변량을 통계량이라고 한다. (확률변수로 취급)\n",
    "- 추정량(Estimator): 모수의 추정을 위해 사용하는 통계량(이나 통계량의 함수) (통계량의 부분집합)\n",
    "- 추정치(Estimate): 주어진 표본값을 대입해서 구해진 추정량의 특정값 (추정량의 원소. 추정량 중에서 실현된 수치값, 상수값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 추정의 방법\n",
    "\n",
    "모수를 추정하는 방법 중 아래 두 가지를 살펴보자\n",
    "- 적률을 사용하는 방법\n",
    "- 가능도원리(likelihood principle)을 사용하는 방법\n",
    "\n",
    "### 4.1.1 적률추정법\n",
    "\n",
    "적률추정의과정  \n",
    "1. 추정의 대상이 되는 모수들이 몇 개인지 파악한다.\n",
    "2. 모수의 갯수만큼 ${\\mu_j}'={m_j}'$ ($j = 1,2, \\cdots , k$. k: 모수의 갯수)의 등식을 설정한다.\n",
    "3. 연립방정식을 풀어서 모수에 관한 식으로 정리한다.\n",
    "4. 위 연립방정식의 해가 각 모수에 대한 적률추정량(Moment Estimator)이다.\n",
    "\n",
    "### 4.1.2 최대가능도 추정법(Maximum likelihood estimation)\n",
    "\n",
    "확률변수 $X_1, X_2, \\cdots , X_n$의 결합확률밀도함수가 $f(x_1, \\cdots, x_n; \\theta)$라고 하자. 결합확률밀도함수 $f(x_1, \\cdots, x_n; \\theta)$는 고정된 모수 $\\theta$에 대하여 $(x_1, \\cdots, x_n)$의 함수로 사용된다. 그러나 반대로 $f(x_1, \\cdots, x_n; \\theta)$를 관측치 $X_1 = x_1, X_2 = x_2, \\cdots , X_n = x_n$이 주어졌을 때 모수 $\\theta(\\in \\Omega)$의 함수로 생각해 볼 수도 있다. 즉 다음과 같이 \n",
    "\n",
    "$$\n",
    "L(\\theta) = L(\\theta; x_1, x_2, \\cdots , x_n) = f(x_1, x_2, \\cdots , x_n; \\theta)\n",
    "$$\n",
    "\n",
    "로 표기하고 이를 $X_1, X_2, \\cdots , X_n$의 가능도함수(likelihood function)라고 한다. 즉, 가능도함수 $L(\\theta)$는 주어진 자료 $(x_1, x_2, \\cdots , x_n)$에 대하여 그것이 얻어질 가능성을 모수 $\\theta$에 대한 함수로 나타낸 것이라 할 수 있다. \n",
    "\n",
    "참고로 가능도함수는 $\\theta$의 함수이므로 확률밀도함수가 아니라는 점을 유의하자. \n",
    "\n",
    "확률변수 $X_1, X_2, \\cdots , X_n$이 서로 독립이고 $X_i$가 확률밀도함수 $f_i(x_i;\\theta)$를 갖는다고 하면 $X_1, X_2, \\cdots , X_n$의 결합확률밀도함수는 \n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^{n} f_i(x_i;\\theta) = f_1(x_1; \\theta)f_2(x_2; \\theta) \\cdots f_n(x_n; \\theta)\n",
    "$$\n",
    "\n",
    "이며, 가능도함수는\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\theta; x_1, \\cdots , x_n) & = \\prod_{i=1}^{n} f_i(x_i; \\theta) \\\\\n",
    "                             & = f_1(x_1; \\theta)f_2(x_2; \\theta) \\cdots f_n(x_n; \\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "로 나타낼수 있다. 따라서 $X_1, X_2, \\cdots , X_n$이 확률밀도함수 $f(x;\\theta)$로부터의 랜덤표본이라고 하면 $f_i(x_i;\\theta) = f(x_i;\\theta)$이므로, 가능도 함수는\n",
    "\n",
    "$$\n",
    "L(\\theta; x_1, \\cdots , x_n) = \\prod_{i=1}^{n} f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "가 된다. 이 가능도함수를 최대화하는 통계량을 사용하여 모수를 추정하는 방법을 아래 정의에서 설명한다. \n",
    "\n",
    "**\\[정의 4.2\\]**  \n",
    "랜덤표본의 가능도함수 $L(\\theta;x_1, x_2, \\cdots , x_n)$을 최대화하는 $\\theta$의 값을 $\\hat{\\theta} = \\hat{\\theta}(x_1, x_2, \\cdots , x_n) \\in \\Omega$라고 할 때, $\\hat{\\theta} = \\hat{\\theta}(X_1, X_2, \\cdots , X_n) \\in \\Omega$을 모수 $\\theta$의 최대가능도 추정량(maximum likelihood estimator)이라 한다. \n",
    "\n",
    "이 최대가능도 추정량의 의미는 '실제로 관측된' 자료가 얻어질 확률을 가장 높게 만드는(즉, 주어진 관측값을 가장 잘 설명하는) 가능도함수의 $\\theta$값을 모수 $\\theta$의 추정량으로 삼는 것이다. (이 추정량은 랜덤표본에 대한 확률변수들로 이루어진 함수로 표현된다)\n",
    "\n",
    "예를 들어, $X_1, \\cdots , X_5$가 서로 독립인 베르누이(p) 확률변수라고 하자. 이때 $Y=\\sum_{}^{}X_i$는 $B(5,p)$ 분포를 따르며, Y의 확률밀도함수는 몇가지 다른 p에 대해서 다음과 같이 주어진다.\n",
    "<img src=\"./images/binom_dist_table.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "즉 이 표는 Y, p 평면상에 $\\binom{5}{x}p^x(1-p)^{5-x}$의 값을 그려놓은 3차원 그래프처럼 생각해볼 수 있고, 가능도 함수는 Y=y로 주어졌을 때 모수 p에 대한 함수이므로 Y=3으로 주어지면 위 그림에서 빨간색 음영부분에 해당하며, 이는 Y=3에서 자른 그래프의 단면으로 이해하면 된다. 이 단면 그래프에서는 p=0.6에서 가능도 함수값이 0.34560으로 최대가 되며 따라서 최대가능도 추정값은 0.6이다. \n",
    "\n",
    "그런데 가능도함수 $L(\\theta;x_1, x_2, \\cdots , x_n)$을 최대화하는 $\\theta$의 값을 찾는 문제는 로그가능도함수\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "log L(\\theta; x_1, x_2, \\cdots , x_n) & = log \\prod_{i=1}^{n} f_i(x_i; \\theta) \\\\\n",
    "                                      & = \\sum_{i=1}^{n} log\\ f_i(x_i; \\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "를 최대화하는 $\\theta$의 값을 찾는 것과 같다($\\because$ 로그함수는 단조증가함수). \n",
    "\n",
    "로그가능도함수를 이용하는 이유: 랜덤표본의 가능도함수는 주변 확률밀도합수의 곱으로 표현되는데 여기에 로그를 취하면 합이 되므로 미분 계산도 쉬워지고 컴퓨터에서 연산할 때 안정성도 높아진다. \n",
    "\n",
    "많은 경우 로그가능도함수를 최대화하는 문제는 \n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} log L(\\theta; x_1, x_2, \\cdots , x_n) = 0\n",
    "$$\n",
    "\n",
    "의 해를 구하는 문제로 귀착된다. 물론 위 식을 만족하는 값이 곧 로그가능도함수를 최대화한다는 보장은 없으며, 최대화 여부를 2차 미분 등을 통해 가려야 한다.\n",
    "\n",
    "정규분포로 예를 들어 설명해보자. $X_1, X_2, \\cdots , X_n$을 $N(\\mu, \\sigma^2)$으로 부터 얻은 랜덤표본이라고 하자. 이때 가능도함수는 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mu, \\sigma^2; x_1, x_2, \\cdots , x_n) & = \\prod_{i=1}^{n} f(x_i; \\mu, \\sigma^2) \\\\\n",
    "                                         & = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} e^{-\\sum_{i=1}^{n}(x_i-\\mu)^2/2\\sigma^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "이며, 따라서 로그가능도함수는\n",
    "\n",
    "$$\n",
    "log L(\\mu, \\sigma^2; x_1, x_2, \\cdots , x_n) = -{n \\over 2}log(2\\pi\\sigma^2)-\\sum_{i=1}^{n}\\frac{(x_i-\\mu)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "이다. 이 로그가능도함수를 최대화하는 $(\\mu, \\sigma^2)$값은 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{d}{d\\mu} log L(\\mu, \\sigma^2; x_1, x_2, \\cdots , x_n) = \\sum_{i=1}^{n} \\frac{(x_i-\\mu)}{\\sigma^2} = 0 \\\\\n",
    "& \\frac{d}{d\\sigma^2} log L(\\mu, \\sigma^2; x_1, x_2, \\cdots , x_n) = -\\frac{n}{2\\sigma^2}+\\frac{\\sum_{i=1}^{n}(x_i-\\mu)^2}{2\\sigma^4} = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "의 두 연립방정식의 해를 구하면 된다. 이렇게 구한 최대가능도 추정량은\n",
    "\n",
    "$$\n",
    "(\\hat{\\mu}, \\hat{\\sigma}^2) = \\left ( \\overline{X}_n, \\sum_{i=1}^{n} \\frac{(X_i - \\overline{X}_n)^2}{n} \\right )\n",
    "$$\n",
    "\n",
    "이 된다. \n",
    "\n",
    "**\\[정리 4.1\\]**  \n",
    "최대가능도 추정량의 불변성: $X_1, \\cdots , X_n$을 확률밀도함수 $f(x; \\theta), \\theta \\in \\Omega$를 갖는 분포에서 얻은 랜덤표본이라고 하자. $\\hat{\\theta}_n$이 모수 $\\theta$의 최대가능도 추정량이면, $\\theta$의 함수인 $g(\\theta)$에 대하여, $g(\\hat{\\theta}_n)$이 $g(\\theta)$의 최대가능도 추정량이 된다. \n",
    "\n",
    "위에서 $N(\\mu, \\sigma^2)$으로 부터 얻은 랜덤표본에 대한 $\\sigma^2$의 최대가능도 추정량으로 $\\hat{\\sigma^2}=\\sum_{i=1}^{n} \\frac{(X_i - \\overline{X}_n)^2}{n}$을 구하였다 이제, $g(\\sigma^2)=\\sigma$의 최대가능도 추정량은 불변성에 의하여 $g(\\hat{\\sigma^2})=\\hat{\\sigma}$가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 추정의 기준 및 최소분산 비편향추정량\n",
    "\n",
    "용어 정의\n",
    "- 손실함수(Loss Function)\n",
    "- 평균제곱오차(Mean Square Error; MSE)\n",
    "- 편향(Bias)\n",
    "- 비편향추정량(Unbiased Estimator; UE)\n",
    "\n",
    "표본의 함수인 추정량은 확률변수임. 어떤 확률변수도 상응하는 확률분포를 가지므로 추정량도 확률분포가 있다. 이러한 확률분포를 살피는 것은 추정량의 성격을 파악하는데 도움이 된다. \n",
    "\n",
    "예를 들어 $X_1, \\cdots X_n$을 $N(\\mu, \\sigma^2)$으로부터 얻은 랜덤표본이라 하자. 모평균 $\\mu$를 표본평균 $\\overline{X}_n$로 추정할 때 추정량 $\\overline{X}_n$의 확률분포는 $N(\\mu, \\frac{\\sigma^2}{n})$ 분포를 따른다. \n",
    "\n",
    "### 4.2.1 추정의 기준 (Criterion of Estimation)\n",
    "\n",
    "**1) 표준통계학에서 '좋은추정량'을 판별하는 일반적인 기준은 무엇인가요?**\n",
    "\n",
    "모수의 함수 $g(\\theta)$의 추정에서, 추정량 $T(X)=T(X_1, X_2, \\cdots , X_n)$이 추정 대상인 $g(\\theta)$에 가까운 값을 제공한다면 좋은 추정량이라고 할 수 있다. 이때 '가깝다'는 것의 정의는 기준(criterion)에 따라 다르게 결정될 것이다. 기준의 몇 가지 예를 들면,\n",
    " - 절대오차(Absolute Error): $|T(X)-g(\\theta)|$\n",
    " - 제곱오차(Squared Error): $SE = (T(X)-g(\\theta))^2$\n",
    " - 가중제곱오차(Weighted Squared Error): $(T(X)-g(\\theta))^2w(\\theta)$\n",
    "\n",
    "같은 것들이 있으며, 이들을 흔히 손실함수(loss function)라고 한다. 그런데 손실함수는 확률변수이기 때문에 이것을 최소화하는 추정량 T(X)를 찾기는 불가능하며, 따라서 보통은 손실함수의 기대값을 취해서 deterministic하게 만들고 이 값을 기준으로 사용한다. 일반적으로는 \n",
    "\n",
    " - 평균제곱오차(Mean Square Error): $MSE = E(T(X)-g(\\theta))^2$\n",
    "\n",
    "를 추정의 기준으로 많이 사용한다. \n",
    "\n",
    "**2) 위에서 제시된 기준에 가장 적절한 추정량을 찾아내는 방법으로 무엇이 있을까요?(정의4.3)**\n",
    "\n",
    "MSE는 모수 $\\theta$에 대한 함수이기 때문에 이를 최소화하는 추정량 T(X)를 찾는 것이 항상 가능하지는 않다. \n",
    "\n",
    "추정을 더 효율적(?)으로 하기 위해, 비상식적이고 편협된 부적절한 추정량들을 고려대상에서 제외하는 것이 합리적일 것이다. 따라서 아래에서 소개할 비편향추정량(unbiased estimator)들의 집합을 우선 고려하고, 그 안에서만 평균제곱오차를 최소화하는 추정방법을 찾는 것이 좋다. \n",
    "\n",
    "**\\[정의 4.3\\]**  \n",
    "T(X)를 $g(\\theta)$의 추정량이라고 할 때, $E[T(X)]-g(\\theta)$를 T(X)의 편향(bias)이라고 하며 $E[T(X)]=g(\\theta)$(즉 편향=0)이면 T(X)를 $g(\\theta)$의 비편향추정량이라고 한다. 추정량 T(X)의 분산은 $E[T(X)-E[T(X)]]^2$이다. \n",
    "\n",
    "즉, 어떤 추정량이 비편향추정량이라면 그 기대값이 추정하려는 모수와 같기 때문에, 추정의 대상을 과대/과소추정하는 경향이 없이 평균적으로 목표치를 잘 맞추고 있다는 뜻이 된다. \n",
    "\n",
    "예를 들어, $X_1, X_2, \\cdots X_{10}$을 $N(\\mu, \\sigma_0^2)$ ($\\sigma_0^2$은 알려진 값)으로부터 얻은 랜덤표본이라고 할 때, $T_1(X)=\\overline{X}_{10}$와 $T_2(X)={(X_1+X_2) \\over 2}$는 $E[T_1(X)] = E[T_2(X)] = \\mu$이므로 모두 모평균 $\\mu$의 비편향추정량이다. \n",
    "\n",
    "**3) 위에서 제시된 기준을 구성하는 요소는 무엇과 무엇이 있나요?(정리4.2)**\n",
    "\n",
    "그런데 추정의 기준이 되는 평균제곱오차(MSE)는 다음 정리와 같이 분산과 편향의 제곱의 합으로 구성할 수 있다.\n",
    "\n",
    "**\\[정리 4.2\\]**  \n",
    "모수의 함수 $g(\\theta)$의 추정량 $T(X)$의 평균제곱오차(MSE)는 \n",
    "\n",
    "$$\n",
    "MSE = E[T(X)-g(\\theta)]^2 = Var(T(X)) + (bias)^2\n",
    "$$\n",
    "\n",
    "을 만족한다. 즉, MSE를 작게 하는 추정량은 편향의 절대값과 분산 둘 다 작은 값을 가져야 한다는 것이다. 비편향추정량들만의 집합을 고려할 때는, MSE를 최소화하는 추정량을 구하기 위해 분산을 최소화(가능하면 모든 $\\theta$에 대해서 균일하게)하는 추정량을 구하면 된다. 비편향추정량 중에서 가장 좋은 것을 고르기 위해서는 다음과 같이 분산의 비(variance ratio)를 이용하면 편하다.\n",
    "\n",
    "**\\[정의 4.4\\]**  \n",
    "$T_1(X)$와 $T_2(X)$가 $g(\\theta)$의 비편향추정량일 때 그 분산의 비, 즉 $\\frac{Var[T_2(X)]}{Var[T_1(X)]}$를 추정량 $T_1(X)$와 $T_2(X)$에 대한 상대효율(relative efficiency)이라고 한다. \n",
    "\n",
    "$T_1(X)$의 분산이 $T_2(X)$의 분산보다 작으면 MSE 기준에 따라 $T_1(X)$가 더 좋은 추정량이며, $T_1(X)$의 $T_2(X)$에 대한 상대효율은 1보다 큰 값을 갖게 된다. 예를 들어, 바로 위에서 살펴본 예제에서 $T_1(X)=\\overline{X}_{10}$와 $T_2(X)={(X_1+X_2) \\over 2}$의 분산은 각각 $Var(T_1(X))={\\sigma_0^2 \\over 10}$와 $Var(T_2(X))={\\sigma_0^2 \\over 2}$이다. 따라서 추정량 $T_2(X)$의 $T_1(X)$에 대한 상대효율은 2/10=0.2이며, $T_1(X)$이 $T_2(X)$보다 효율이 높으므로 $T_1(X)$이 더 좋은 추정량임을 알 수 있다. \n",
    "\n",
    "두 개의 주어진 비편향추정량은 상대효율로 비교할 수 있으나 비편향추정량은 여러 개가 존재할 수 있고 모든 비편향추정량을 한꺼번에 비교하기는 어렵다. 그러면 어떻게하면 효율이 가장 높은 비편향추정량을 찾을 수 있을까? 이것이 바로 다음부터 소개할 최소분산 비편향추정량을 찾는 문제가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 최소분산 비편향추정량 (Minimum Variance Unbiased Estimator)\n",
    "\n",
    "**\\[정의 4.5\\]**  \n",
    "확률변수 $X_1, X_2, \\cdots , X_n$의 결합 확률밀도함수가 $f(x_1, x_2, \\cdots , x_n;\\theta)$라고 하자. 함수 $g(\\theta)$의 추정량 $T^*(X)$가 다음의 두 조건 \n",
    "\n",
    "(1) $T^*(X)$는 $g(\\theta)$의 비편향추정량이다. (즉, $E[T^*(X)]=g(\\theta)$)  \n",
    "(2) $g(\\theta)$의 임의의 비편향추정량 $T(X)$에 대해서 $Var(T^*(X)) \\leq Var(T(X))$를 만족한다. \n",
    "\n",
    "를 만족시키면 이를 $g(\\theta)$의 최소분산 비편향추정량(Minimum Variance Unbiased Estimator; MVUE)이라고 한다. \n",
    "\n",
    "- 최소분산 비편향추정량을 구하는 (교재에제시된) 두 가지 방법\n",
    " - 크래머-라오 방법(Cramer-Rao Lower Bound Method)\n",
    " - 완비충분통계량 방법(Complete Sufficient Statistic Method)\n",
    "\n",
    "**1) 크래머-라오방법(Cramer-Rao Lower Bound Method)**\n",
    "\n",
    "- 크래머라오 방법은(자료의 관측값이 주어진 상태에서 모수에 대한 가능도함수의 정보량을 표현하는) 피셔정보식 $I(\\theta)$를 활용해, 최소분산 비편향추정량을 구하는 방법이다.\n",
    "- 비편향추정량(UE)은 피셔정보식 $I(\\theta)$의 특정 함수보다 더 작은 분산을 가질 수 없다는 정리를 크래머-라오부등식, 또는 정보부등식(정리4.3)이라 합니다. 그리고 부등식이 가질 수 있는 최소값($I(\\theta)$의 함수)을 크래머-라오 하한값(Cramer-Rao Lower Bound)이라 합니다.\n",
    "- 정보부등식에 따르자면 비편향추정량의 분산은 크래머-라오 하한값의 함수와 같거나 큽니다. 그러므로 비편향추정량의 분산이 크래머-라오 하한값과 같다면, 그 비편향추정량은 분산이 가장 작은 최소분산 비편향추정량이됩니다.\n",
    "\n",
    "크래머-라오의 정보부등식은 적절한 조건하에서 비편향추정량이 가질 수 있는 분산의 하한값(크래머-라오 하한값)을 제공한다. 이는 어떠한 비편향추정량도 이 하한값보다 작은 값을 가질 수 없다는 뜻으로, 이 하한값을 분산으로 가지는 비편향추정량이 있다면 이 추정량이 바로 최소분산 비편향추정량이라는 의미이다. \n",
    "\n",
    "우선 정보부등식에 사용되는 피셔의 정보(Fisher's information)에 대해 알아보자. 피셔의 정보 $I(\\theta)$는 확률변수 X의 확률밀도함수가 $f(x;\\theta)$일 때 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "I(\\theta) = E[({\\partial \\over \\partial \\theta } \\log f(X;\\theta))^2]\n",
    "$$\n",
    "\n",
    "한번 정규확률분포에 대한 피셔의 정보 $I(\\mu)$를 구하는 법을 연습해보자. $X \\sim N(\\mu, \\sigma_0^2)$ ($\\sigma_0^2$은 알려진 값)이라고 할 때, 확률밀도함수는 \n",
    "\n",
    "$$\n",
    "f(x; \\mu) = \\frac{1}{\\sqrt{2\\pi}\\sigma_0^2} exp \\left [\\frac{-(x-\\mu)^2}{2\\sigma_0^2} \\right ]\n",
    "$$\n",
    "\n",
    "이며, 따라서 피셔의 정보는\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I(\\mu) & = E \\left [ {\\partial \\over \\partial \\mu } \\log f(X;\\mu) \\right ]^2 \\\\\n",
    "       & = E \\left [ \\frac{(X-\\mu)}{\\sigma_0^2} \\right ]^2 \\\\\n",
    "       & = \\frac{1}{\\sigma_0^2} \\qquad (\\because E(X^2)=\\mu^2+\\sigma_0^2,\\ E(X) = \\mu)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "로 주어진다. 여기에서 알 수 있는 것은 $\\sigma_0^2$가 클수록 피셔의 정보(하나의 관찰값 X가 모평균 $\\mu$에 대하여 가지는 정보)가 작아진다는 점과 $I(\\mu)$가 $\\mu$ 값에 의존하지 않는다는 점이다. 분산이 작으면 관측값이 $\\mu$에 가까이 있을 확률이 더 높기 때문 관측값이 모평균에 대하여 가지는 정보가 커진다. 또한 관측값이 모평균에 대하여 가지는 정보는 모평균이 어디에 있든 간에 분산에만 의존한다는 것은 정규분포의 형태적 특성을 고려하면 당연한 이치이다. \n",
    "\n",
    "이제, $X_1, X_2, \\cdots , X_n$이 확률밀도함수가 $f(x;\\theta), \\theta \\in \\Omega$인 분포로부터 얻은 랜덤표본이라고 하자. 그러면 $g(\\theta)$의 비편향추정량 $T(X)=T(X_1, X_2, \\cdots , X_n)$의 분산에 대한 부등식은 다음과 같이 주어진다. \n",
    "\n",
    "**\\[정리 4.3\\]**  \n",
    "정보부등식(Information Inequality): 확률밀도함수 $f(x;\\theta)$와 통계량 $T(X)$에 대해 몇 가지 가정(책 1, 2, 3 가정)을 만족한다고 가정한다고 하자. 모든 $\\theta \\in \\Omega$에 대하여 $Var(T(X))<\\infty ,\\ E(T(X))=g(\\theta),\\ 0<I(\\theta)<\\infty$라고 하면, $g(\\theta)$는 미분가능하며\n",
    "\n",
    "\n",
    "$$\n",
    "Var(T(X)) \\geq \\frac{[g'(\\theta)]^2}{nI(\\theta)}\n",
    "$$\n",
    "\n",
    "이 성립한다. 간단함을 위해 $g(\\theta)$를 $\\theta$라고 하면 $\\frac{\\partial}{\\partial \\theta} g(\\theta) = 1$ 이므로\n",
    "\n",
    "$$\n",
    "Var(T(X)) \\geq \\frac{1}{nI(\\theta)}\n",
    "$$\n",
    "\n",
    "이다. 따라서 $Var(T(X)) = \\frac{1}{nI(\\theta)}$를 만족하는 추정량 T(X)는 비편향추정량 가운데 가장 작은 분산을 가지며, 이것을 $\\theta$의 최소분산 비편향추정량(MVUE)라고 한다. \n",
    "\n",
    "예를 들어보자. $X_1, X_2, \\cdots , X_n$을 $POI(\\lambda)$ 분포로부터 구한 랜덤표본이라고 하자. 우선 피셔의 정보 $I(\\lambda)$를 계산하면 다음과 같다. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I(\\lambda) & = E \\left [ \\frac{\\partial }{\\partial \\lambda} \\log f(X;\\lambda) \\right ]^2 \\\\\n",
    "           & = E \\left [ \\frac{\\partial }{\\partial \\lambda} \\log \\frac{e^{-\\lambda}\\lambda^X}{X!} \\right ]^2 \\\\\n",
    "           & = E \\left [ \\frac{\\partial }{\\partial \\lambda} (\\log e^{-\\lambda}\\lambda^X - \\log X!) \\right ]^2 \\\\\n",
    "           & = E \\left [ \\frac{-e^{-\\lambda}\\lambda^X + Xe^{-\\lambda}\\lambda^{X-1}}{e^{-\\lambda}\\lambda^X} - 0 \\right ]^2 \\qquad \\left (\\because \\frac{\\partial }{\\partial x}\\log f(x)= \\frac{f'(x)}{f(x)} \\right ) \\\\\n",
    "           & = E \\left [ \\frac{-\\lambda^X + X\\lambda^{X-1}}{\\lambda^X} \\right ]^2 \\\\\n",
    "           & = E \\left [ -1 + {X \\over \\lambda} \\right ]^2 \\\\\n",
    "           & = E \\left [ {X - \\lambda \\over \\lambda} \\right ]^2 \\\\\n",
    "           & = \\frac{E(X^2)-2\\lambda E(X) + \\lambda^2}{\\lambda^2} \\\\\n",
    "           & = \\frac{1}{\\lambda} \\qquad (\\because E(X^2) = \\lambda^2+\\lambda,\\ E(X) = \\lambda)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "그런데 $Var(\\overline{X}_n)={\\lambda \\over n}$이므로, 비편향추정량 $\\overline{X}_n$은 $Var(\\overline{X}_n)=\\frac{1}{nI(\\lambda)}$를 만족한다. 따라서 표본평균 $\\overline{X}_n$는 균일 최소분산 비편향추정량이다. \n",
    "\n",
    "다른 예를 들어보자. $X_1, X_2, \\cdots , X_n$을 $N(\\mu, \\sigma^2)$으로부터 얻은 랜덤표본이라고 하자. 이때 피셔의 정보는 $I(\\mu)=1/\\sigma^2$이고, $Var(\\overline{X}_n)=\\sigma^2/n$이므로, $\\overline{X}_n$는 균일 최소분산 비편향추정량이다. \n",
    "\n",
    "**2) 완비충분통계량방법(Complete Sufficient Statistic Method)**\n",
    "\n",
    "비편향추정량 중에서 최소분산을 가지는 추정량을 구하는 두 번째 방법은 바로 완비충분통계량방법이다. \n",
    "\n",
    "**\\[정의 4.6\\]**  \n",
    "$X_1, X_2, \\cdots , X_n$을 $f(x;\\theta)$로부터의 확률표본이라 하자. 어떤 통계량 $S=u(X_1, X_2, \\cdots , X_n)$에 대해, $S=s$일 때 $X=(X_1, X_2, \\cdots , X_n)$의 조건부 확률변수 $X|S=s$의 분포가 $\\theta$에 의존하지 않는다면 S를 $\\theta$에 대한 충분통계량이라 한다. \n",
    "\n",
    "통계량 s = S(X)가 주어졌을 때, 데이터 X의 조건부확률 분포가 파라미터 θ에 의존하지 않으면, 다시말해 아래 식\n",
    "\n",
    "$$\n",
    "\\Pr(x\\mid s,\\theta )=\\Pr(x\\mid s) \n",
    "$$\n",
    "\n",
    "이 성립하면 통계량 s = S(X)가 underlying parameter θ에 대해 sufficient하다고 한다. 참고로 A, B가 독립이면 $P(A\\mid B)=P(A)$이므로 $X|S=s$와 $\\theta$가 독립이면 $P(X\\mid S=s, \\theta)=P(X\\mid S=s)$이다.\n",
    "\n",
    "이 정의는, 일단 충분통계량의 값이 주어지면 표본의 분포는 모수에 의존하지 않으므로, 이 경우에는 표본이 더 이상 모수에 대한 아무런 정보를 가지고 있지 않다는 것을 의미한다. 그러므로 표본은 모수추정에 이용될 근거를 잃어버리게 되며, 모수추정과 관계없는 정보만이 남는다는 것이다. 따라서 확률변수 ($X_1, X_2, \\cdots , X_n$)이 관찰되었을 때, 모수 $\\theta$에 대한 정보는 모두 충분통게량 S(X)에 포함되어 있다고 할 수 있다. (sufficiency의 정의 참고: [1](https://onlinecourses.science.psu.edu/stat414/node/282/), [2](https://ocw.mit.edu/courses/economics/14-381-statistical-method-in-economics-fall-2013/lecture-notes/MIT14_381F13_lec4.pdf), [3](http://internet.math.arizona.edu/~tgk/466/sufficient.pdf))\n",
    "\n",
    "충분통계량의 개념을 이해하기 위해 예를 들어 설명해보자. $X=(X_1, X_2, \\cdots , X_n)$이 베르누이 $B(1, p)$로부터 구한 랜덤표본이라고 할 때, $S=\\sum_{i=1}^{n}X_i$는 모수 $p$에 대한 충분통계량임을 보이고자 한다. 우선, $S \\sim B(n,p)$이므로 확률밀도함수가 \n",
    "\n",
    "$$\n",
    "f(s;p) = \\binom{n}{s}p^s(1-p)^{n-s} \\qquad (s = 0,1, \\cdots , n)\n",
    "$$\n",
    "\n",
    "로 주어진다. $X|S=s$의 조건부 확률밀도함수는 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x_1, x_2, \\cdots , x_n \\mid s) & = P(X_1=x_1, \\cdots , X_n=x_n \\mid S=s) \\\\\n",
    "& = \\frac{P(X_1=x_1, \\cdots , X_n=x_n, S=s)}{P(S=s)}, \\quad S \\sim B(n, p) \\\\\n",
    "& = \\frac{p^s(1-p)^{n-s}}{\\binom{n}{s}p^s(1-p)^{n-s}}, \\quad s = \\sum_{i=1}^{n}x_i \\\\\n",
    "& = \\frac{1}{\\binom{n}{s}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "가 되며, 이는 모수 p에 의존하지 않으므로 $S=\\sum_{}^{}X_i$는 $\\theta$에 대한 충분통계량이 된다. (좀 더 디테일한 설명은 [여기](http://contents.kocw.net/KOCW/document/2015/chungbuk/najonghwa/5.pdf)를 참고)\n",
    "\n",
    "하지만 정의 4.6을 이용해 주어진 통계량이 충분한지를 파악하려면 S의 분포가 요구되며(위 예제에서는 $f(s;p) = \\binom{n}{s}p^s(1-p)^{n-s}$ 였다) 조건부 확률밀도함수를 구하는 복잡한 과정을 필요로 하기 때문에 다음에 다룰 인수분해정리를 사용하면 더 간단히 확인할 수 있다. \n",
    "\n",
    "**\\[정리 4.4\\]**  \n",
    "인수분해정리(Factorization Theorem): $X_1, X_2, \\cdots , X_n$의 결합 확률밀도함수가 $f(x_1, x_2, \\cdots , x_n; \\theta)$일 때 $S(X)=(S_1(X), \\cdots , S_k(X))$를 k개의 통계량이라고 하자. 이때 S가 결합 충분통계량일 필요충분조건은 결합 확률밀도함수가 $s$와 $\\theta$만의 함수인 g와, $(x_1, x_2, \\cdots , x_n)$ 만의 함수인 h와의 곱의 꼴로 다음과 같이 나타내어지는 것이다. \n",
    "\n",
    "$$\n",
    "f(x_1, x_2, \\cdots , x_n;\\theta) = g(s(x);\\theta)\\times h(x_1, x_2, \\cdots , x_n)\n",
    "$$\n",
    "\n",
    "예를 들어, $X_1, X_2, \\cdots , X_n$이 포아송분포($\\lambda$)에서의 랜덤샘플이라고 할 때, 파라미터 $\\lambda$의 충분통계량을 구해보자. $X_1, X_2, \\cdots , X_n$은 랜덤표본이므로 $X_1, X_2, \\cdots , X_n$의 결합확률질량함수 pmf는 \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "f(x_1, x_2, \\cdots , x_n; \\lambda) & = f(x_1;\\lambda) \\times f(x_2;\\lambda) \\times \\cdots \\times f(x_n;\\lambda)  \\\\\n",
    "& = \\frac{e^{-\\lambda}\\lambda^{x_1}}{x_1!} \\times \\frac{e^{-\\lambda}\\lambda^{x_2}}{x_2!} \\times \\cdots \\times \\frac{e^{-\\lambda}\\lambda^{x_n}}{x_n!} \\\\\n",
    "& = (e^{-n\\lambda}\\lambda^{\\sum x_i}) \\times \\left ( \\frac{1}{x_1!x_2! \\cdots x_n!} \\right ) \\\\\n",
    "& = g(s(\\sum_{}^{} x_i);\\lambda)\\times h(x_1, x_2, \\cdots , x_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "이다. 위에서 보는 바와 같이 결합 pmf를 두 부분(통계량 $Y=\\sum_{i=1}^{n} X_i$만의 함수인 g와 파라미터 $\\lambda$에 의존하지 않는 함수 h)으로 나눌 수 있다. 그러므로 인수분해 정리에 의해 $Y=\\sum_{i=1}^{n} X_i$는 $\\lambda$에 대한 충분통계량이다. \n",
    "\n",
    "추가로 $Y=\\overline{X}_n$도 $\\lambda$의 충분통계량임을 알 수 있는데, 이로부터 충분통계량은 하나 이상일 수 있다는 사실과 Y가 충분통계량이면 Y의 일대일함수도 역시 충분통계량이라는 점을 알 수 있다. \n",
    "\n",
    "이번에는 정규분포의 랜덤표본으로부터 모평균, 모분산에 대한 충분통계량을 구해보자. $X_1, X_2, \\cdots , X_n$이 $N(\\mu, \\sigma^2)$으로부터 구한 랜덤표본이라고 할 때 이들의 결합확률밀도함수는\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, \\cdots , x_n;\\mu, \\sigma^2) = \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n} exp \\left [ -\\frac{\\sum_{i=1}^{n}(x_i-\\mu)^2}{2\\sigma^2} \\right ]\n",
    "$$\n",
    "\n",
    "으로 주어진다. 그런데 이는\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x_1, x_2, \\cdots , x_n;\\mu, \\sigma^2) & = \\frac{1}{(\\sqrt{2\\pi}\\sigma)^n} exp \\left [ -\\frac{\\sum_{i=1}^{n}x^2 -2\\mu \\sum_{i=1}^{n}x_i + n\\mu^2}{2\\sigma^2} \\right ] \\cdot 1 \\\\\n",
    "& = g(s_1, s_2; \\mu, \\sigma^2) \\cdot h(x_1, x_2, \\cdots , x_n) \\qquad (s_1=\\sum_{i=1}^{n}x_i,\\ s_2=\\sum_{i=1}^{n}{x_i}^2,\\ h(x_1, x_2, \\cdots , x_n)=1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "와 같이 표현되므로 정리 4.4로부터 $S_1=\\sum_{i=1}^{n}X_i$와 $ S_2=\\sum_{i=1}^{n}{X_i}^2$은 결합충분통계량이다. 나아가 $S_1$과 $S_2$의 1:1 함수인 $\\overline{X}_n$과 $\\frac{\\sum_{i=1}^{n}(X_i-\\overline{X}_n)^2}{(n-1)}$도 역시 결합충분통계량이 되므로 표본평균과 표본분산은 각각 모평균과 모분산에 대한 충분통계량이라는 것을 알 수 있다. ([참고자료](https://onlinecourses.science.psu.edu/stat414/node/283/)에서 2번째 Example을 보자)\n",
    "\n",
    "추가로, 충분통계량은 유일하지 않으므로 모수에 관한 정보를 잃지 않으면서 자료를 가장 많이 축소시키는 충분통계량이 선호된다. 즉, 모수의 추정에 필요없는 정보를 가장 적게 가지고 있는 충분통계량인 최소 충분통계량(minimal sufficient statistic)이 좋다는 말이다.\n",
    "\n",
    "**\\[정리 4.5\\]**  \n",
    "라오-블랙웰 정리: 모수의 함수 $g(\\theta)$에 대하여 S가 충분통계량이고, $T(X)$가 그의 한 비편향추정량이라고 하자. 이때 $\\delta(S)=E(T(X)\\mid S)$라고 하면 $\\delta(S)$도 역시 $g(\\theta)$의 비편향추정량이며, 모든 $\\theta$에 대하여\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var(\\delta(S)) & = E[(\\delta(S)-g(\\theta))^2] \\\\\n",
    "               & \\leq E[(T(X)-g(\\theta))^2] \\\\\n",
    "               & = Var(T(X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "가 성립한다. \n",
    "\n",
    "위 정리의 내용은 어떤 비편향추정량 T(X)가 존재하면 충분통계량 S에 대한 조건부 기대값 E[T(X)|S] 역시 비편향추정량이며, T(X)보다 작거나 같은 분산을 가진다는(더 좋은 추정량이라는) 의미이다. 이 정리를 이용하면 비편향추정량이 유일한 경우 그것이 바로 최소분산 비편향추정량이 됨을 알 수 있다. 이 유일성을 다루는데 있어 중요한 도구인 완비성에 대해 아래 정리를 통해 알아보자. \n",
    "\n",
    "**\\[정의 4.7\\]**  \n",
    "랜덤표본 $X_1, \\cdots , X_n$으로부터 계산된 통계량 $T=T(X_1, \\cdots , X_n)$에 대하여, $E(g(T))=0$을 모든 $\\theta \\in \\Omega$에 대해 만족하는 $\\theta$에 무관한 함수 $g$가 $g(\\cdot)\\equiv 0$ 뿐이라면, T를 완비통계량(complete statistic)이라고 한다. 만약 T가 $\\theta$에 대한 충분통계량이라면, 이를 완비충분통계량이라 한다. \n",
    "\n",
    "**\\[정리 4.6\\]**  \n",
    "레만-쉐페 정리: 모수 $\\theta$에 대하여 T가 완비 충분통계량이고, S(X)가 $g(\\theta)$의 비편향추정량이라고 하자. 이때 $\\delta(T)=E(S(X) \\mid T)$는 $g(\\theta)$의 유일한 최소분산 비편향추정량(MVUE)이다.\n",
    "\n",
    "퍼실의 코멘트: \n",
    "- 완비충분통계량방법은 비편향추정량이 완비충분성을 가지고 있을 경우(완비충분통계량의 함수일 경우) 그 통계량이 유일한 최소분산 비편향추정량이 된다는 성질을 이용합니다.\n",
    "- 즉, 어떤 비편향추정량이 완비충분통계량의 함수임을 밝히거나, 또는 비편향추정량을 완비충분통계량에 대한 조건부기댓값을 취하면 자동으로 유일한 최소분산비편향추정량이 된다는 의미입니다.\n",
    "- 위의 ‘편리한’ 성질을 이용하기 위해 두 가지의 정리가 필요한데, 각각 라오-블랙웰정리(Rao-Blackwell Theorem), 레만-쉐페정리(Lehmann-Scheffe Theorem)이라합니다.(정리의 결과와 증명이 상당히 어려우므로, 위에 요약해드린 결과만 받아들이셔도 괜찮습니다!)\n",
    "- 다만 통계량의 완비성, 충분성을 밝히는 과정이 쉽지 않기 때문에 일반적으로 지수족(Exponential Family)이라는 분포군을 설정하여 완비충분성을 보입니다.\n",
    "- 지수족에 속한 분포의 확률밀도함수로부터 적절한 통계량을 구하면 완비충분성이 보장됩니다.(정리4.7) 그러므로 지수족통계량의 비편향성만 보이면 ‘자동으로’ 유일한 최소분산 비편향추정량이됩니다.\n",
    "\n",
    "한마디로 완비충분성에 대한 입증은 까다로워서 쓰기 어려운데, 다음에 정의되는 지수족에 대해서는 완비충분성이 보장되므로 쉽게 사용할 수 있다는 얘기임. 더구나 자주 사용되는 많은 확률분포들이 지수족에 속하므로 개꿀이란 말씀.\n",
    "\n",
    "**\\[정의 4.8\\]**  \n",
    "확률밀도함수가 적절한 함수 $a, b, c_i, t_i(i=1, \\cdots , k)$에 대하여, \n",
    "\n",
    "$$\n",
    "f(x;\\theta) = a(\\theta) b(x) exp\\left [ \\sum_{i=1}^{k} c_i(\\theta)t_i(x) \\right ], \\quad -\\infty < x < \\infty, \\quad \\theta=(\\theta_1, \\cdots , \\theta_k)\n",
    "$$\n",
    "\n",
    "로 표현되면 이를 k개의 모수 $\\theta_1, \\cdots , \\theta_k$를 가진 지수족(exponential family)에 속한다고 한다. 여기에서 집합 $\\{x:f(x;\\theta)>0\\}$은 모수 $\\theta$에 의존하지 않는다. \n",
    "\n",
    "각종 분포가 지수족에 속하는지에 대한 추가적인 내용은 [참고자료](https://onlinecourses.science.psu.edu/stat414/node/284/)를 참고하라. 베르누이분포, 정규분포, 지수분포, 베타분포, 감마분포, 음이항분포는 지수족에 속하며, $U(0,\\theta)$나 음지수분포와 같이 x값의 영역이 모수 $\\theta$에 의존하는 경우에는 지수족에 속하지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 추정량의 점근적 성질\n",
    "\n",
    "(고정된 크기(n)의 표본이 아니라) 표본의 크기가 커짐에 따라 가지는 추정량의 점근적(asymptotic) 성질에 대해 살펴보자. \n",
    "\n",
    "**\\[정의 4.9\\]**  \n",
    "모수의 함수 $g(\\theta)$의 추정량 $T_n(X)=T(X_1, X_2, \\cdots , X_n)$이, 임의의 $\\epsilon > 0$에 대하여\n",
    "\n",
    "$$\n",
    "\\lim_{n\\rightarrow \\infty} P(\\mid T_n(X)-g(\\theta)\\mid \\leq \\epsilon) = 1\n",
    "$$\n",
    "\n",
    "을 만족하면 추정량 $T_n(X)$는 일치성(consistency)이 있다고 한다. 이는 표본의 크기가 커지면 $T_n(X)$가 $g(\\theta)$에 확률적으로 수렴한다는 의미(참조: 정의 3.5)로, 표본의 크기가 매우 클 때, 추정량 $T_n(X)$로부터 계산된 추정값은 높은 확률로 참모수값에 매우 가까이 있다는 뜻이다. \n",
    "\n",
    "**\\[정리 4.9\\]**  \n",
    "$T_n(X)$를 $g(\\theta)$의 추정량이라고 할 때, 모든 $\\theta \\in \\Omega$에 대하여\n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} E[T_n(X) - g(\\theta)]^2 = 0\n",
    "$$\n",
    "\n",
    "이 성립하면, $T_n(X)$는 일치성이 있다. \n",
    "\n",
    "그런데 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MSE(T_n) & = E[T_n-g(\\theta)]^2 \\\\\n",
    "         & = Var(T_n)+[E(T_n)-g(\\theta)]^2 \\\\\n",
    "         & = E[T_n-E[T_n]]^2+[E(T_n)-g(\\theta)]^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "이다. 이때, $T_n$이 $g(\\theta)$의 비편향추정량이라면 $E(T_n)-g(\\theta)=0$이므로 $MSE(T_n)=Var(T_n)$이 되고, $\\lim_{n\\rightarrow \\infty} Var(T_n)=0$이 성립하면 $\\lim_{n \\rightarrow \\infty} E[T_n-E[T_n]]^2$가 성립하므로 정리 4.9로부터 추정량 $T_n$의 일치성이 보장된다. 즉, n이 엄청 커지면 MSE=0이 된다는 얘기.\n",
    "\n",
    "### 4.2.4 최대가능도 추정량의 점근적 성질\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
